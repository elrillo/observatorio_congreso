{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbF/1tcc1orZGzDHk5gIci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elrillo/observatorio_congreso/blob/main/datos_twitter_senadores_iniciales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji==1.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPooZP1cwFQr",
        "outputId": "00ff3877-c249-4d40-aca9-4ae1ed3b15c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji==1.2.0\n",
            "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import drive\n",
        "from datetime import timezone\n",
        "import re\n",
        "import emoji\n",
        "utc = timezone.utc\n",
        "\n",
        "stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "\n",
        "\n",
        "# Configurar la autenticación de la API de Twitter\n",
        "consumer_key = 'pVH1sHVtwgxA91CJ3h32qiWgT'\n",
        "consumer_secret = 'TbBH3xkFTex4nyJUvkttaW3j6gOyjXCpIBr1e2i7t7NpMQTcVk'\n",
        "access_token = '104474829-ty3Hm8j2wg1mrKhhUQlJYVIddo2MqI08VQEiP9Vp'\n",
        "access_token_secret = 'TFB7LNniyvPX8Bh5jCPCtCjXMQ8STLUA7n946QFACFwEv'\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n"
      ],
      "metadata": {
        "id": "xOjAiFQeDc47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03b2fa8-e912-4ad3-854b-55457eae77df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lista de usuarios de Twitter\n",
        "users = [\"adeurresti\",\"AKusanovicG\",\"alvaroelizalde\",\"ArayaPedro\",\"CalamaVelasquez\",\"CarmenAravenaA1\",\"chahuan\"]\n",
        "          "
      ],
      "metadata": {
        "id": "fVTv3wN6cuzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fecha a partir de la cual recopilar los tweets (11 de marzo de 2022)\n",
        "start_date = \"2022-03-11\""
      ],
      "metadata": {
        "id": "mk-_DD4u3Xt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recopilar los tweets de los usuarios en un dataframe\n",
        "tweets_list = []\n",
        "\n",
        "for user in users:\n",
        "    # Excluir retweets\n",
        "    tweets = tweepy.Cursor(api.user_timeline, screen_name=user, tweet_mode=\"extended\", exclude_replies=True).items()\n",
        "    for tweet in tweets:\n",
        "        # Verificar si el tweet fue publicado después de la fecha de inicio\n",
        "        if tweet.created_at.replace(tzinfo=timezone.utc) >= pd.to_datetime(start_date).replace(tzinfo=timezone.utc):\n",
        "            # Recopilar la información del tweet\n",
        "            user = tweet.user.screen_name\n",
        "            text = tweet.full_text\n",
        "            date = tweet.created_at\n",
        "            hashtags = [tag['text'] for tag in tweet.entities['hashtags']]\n",
        "            likes = tweet.favorite_count\n",
        "            retweets = tweet.retweet_count\n",
        "            tweet_id = tweet.id_str\n",
        "\n",
        "            # Preprocesamiento del texto\n",
        "            text = text.lower()\n",
        "            text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text) # Eliminar menciones\n",
        "            text = re.sub(r\"http\\S+\", \"\", text) # Eliminar enlaces\n",
        "            text = re.sub(r\"\\d+\", \"\", text) # Eliminar números\n",
        "            words = re.findall(r'\\w+|[¿¡]+|\\S', text)\n",
        "            words = [word.translate(str.maketrans(\"\", \"\", string.punctuation)) if word.isalpha() else word for word in words]\n",
        "            words = [word for word in words if not any(char in emoji.UNICODE_EMOJI for char in word)]\n",
        "            words = [word for word in words if not word in stop_words] # Eliminar stop words\n",
        "            words = [word for word in words if len(word) > 1] # Eliminar letras individuales\n",
        "            words = \" \".join(words)\n",
        "\n",
        "            # Agregar los datos del tweet a la lista de tweets\n",
        "            tweets_list.append([user, text, words, date, hashtags, likes, retweets, tweet_id])\n",
        "\n",
        "# Crear el dataframe a partir de la lista de tweets\n",
        "df = pd.DataFrame(tweets_list, columns=[\"Usuario\", \"Tweet\", \"Palabras_sin_stopwords\", \"Fecha\", \"Hashtags\", \"Likes\", \"Retweets\", \"Tweet_ID\"])\n",
        "\n",
        "\n",
        "# Guardar el dataframe como un archivo CSV en Google Drive\n",
        "drive.mount('/content/drive')\n",
        "csv_path = '/content/drive/MyDrive/datos_twitter/tweets_senadores.csv'\n",
        "df.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "id": "NxQnOjlZONCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c7e219-0fa1-4917-b949-1fa097b523ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.api:Rate limit reached. Sleeping for: 101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users2 = [\"Claupascualgrau\",\"daniel_nunez_a\",\"DignidadFabiola\",\"DoctorJLCastro\",\"Duranasenador\",\"felipekast\",\"fidelsenador\",\"galilea_rodrigo\"]"
      ],
      "metadata": {
        "id": "CptM1gVFen5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recopilar los tweets de los usuarios en un dataframe\n",
        "tweets_list = []\n",
        "\n",
        "for user in users2:\n",
        "    # Excluir retweets\n",
        "    tweets = tweepy.Cursor(api.user_timeline, screen_name=user, tweet_mode=\"extended\", exclude_replies=True).items()\n",
        "    for tweet in tweets:\n",
        "        # Verificar si el tweet fue publicado después de la fecha de inicio\n",
        "        if tweet.created_at.replace(tzinfo=timezone.utc) >= pd.to_datetime(start_date).replace(tzinfo=timezone.utc):\n",
        "            # Recopilar la información del tweet\n",
        "            user = tweet.user.screen_name\n",
        "            text = tweet.full_text\n",
        "            date = tweet.created_at\n",
        "            hashtags = [tag['text'] for tag in tweet.entities['hashtags']]\n",
        "            likes = tweet.favorite_count\n",
        "            retweets = tweet.retweet_count\n",
        "            tweet_id = tweet.id_str\n",
        "\n",
        "            # Preprocesamiento del texto\n",
        "            text = text.lower()\n",
        "            text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text) # Eliminar menciones\n",
        "            text = re.sub(r\"http\\S+\", \"\", text) # Eliminar enlaces\n",
        "            text = re.sub(r\"\\d+\", \"\", text) # Eliminar números\n",
        "            words = re.findall(r'\\w+|[¿¡]+|\\S', text)\n",
        "            words = [word.translate(str.maketrans(\"\", \"\", string.punctuation)) if word.isalpha() else word for word in words]\n",
        "            words = [word for word in words if not any(char in emoji.UNICODE_EMOJI for char in word)]\n",
        "            words = [word for word in words if not word in stop_words] # Eliminar stop words\n",
        "            words = [word for word in words if len(word) > 1] # Eliminar letras individuales\n",
        "            words = \" \".join(words)\n",
        "\n",
        "            # Agregar los datos del tweet a la lista de tweets\n",
        "            tweets_list.append([user, text, words, date, hashtags, likes, retweets, tweet_id])\n",
        "\n",
        "# Crear el dataframe a partir de la lista de tweets\n",
        "df = pd.DataFrame(tweets_list, columns=[\"Usuario\", \"Tweet\", \"Palabras_sin_stopwords\", \"Fecha\", \"Hashtags\", \"Likes\", \"Retweets\", \"Tweet_ID\"])\n",
        "\n",
        "# Guardar el dataframe como un archivo CSV en Google Drive\n",
        "drive.mount('/content/drive')\n",
        "csv_path = '/content/drive/MyDrive/datos_twitter/tweets_senadores2.csv'\n",
        "df.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r45SKh68fmm_",
        "outputId": "8f7bce33-ad5f-47de-f0c7-01f7a2946442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.api:Rate limit reached. Sleeping for: 303\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users3 = [\"gastonsaavedra\",\"gustavosanhuez\",\"iallendebussi\",\"ifloressenador\",\"Insulza\",\"ivanmoreirab\",\"jacoloma\",\"javiermacaya\",\"jgarciaruminot\"]"
      ],
      "metadata": {
        "id": "25rQOZzfe5jO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recopilar los tweets de los usuarios en un dataframe\n",
        "tweets_list = []\n",
        "\n",
        "for user in users3:\n",
        "    # Excluir retweets\n",
        "    tweets = tweepy.Cursor(api.user_timeline, screen_name=user, tweet_mode=\"extended\", exclude_replies=True).items()\n",
        "    for tweet in tweets:\n",
        "        # Verificar si el tweet fue publicado después de la fecha de inicio\n",
        "        if tweet.created_at.replace(tzinfo=timezone.utc) >= pd.to_datetime(start_date).replace(tzinfo=timezone.utc):\n",
        "            # Recopilar la información del tweet\n",
        "            user = tweet.user.screen_name\n",
        "            text = tweet.full_text\n",
        "            date = tweet.created_at\n",
        "            hashtags = [tag['text'] for tag in tweet.entities['hashtags']]\n",
        "            likes = tweet.favorite_count\n",
        "            retweets = tweet.retweet_count\n",
        "            tweet_id = tweet.id_str\n",
        "\n",
        "            # Preprocesamiento del texto\n",
        "            text = text.lower()\n",
        "            text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text) # Eliminar menciones\n",
        "            text = re.sub(r\"http\\S+\", \"\", text) # Eliminar enlaces\n",
        "            text = re.sub(r\"\\d+\", \"\", text) # Eliminar números\n",
        "            words = re.findall(r'\\w+|[¿¡]+|\\S', text)\n",
        "            words = [word.translate(str.maketrans(\"\", \"\", string.punctuation)) if word.isalpha() else word for word in words]\n",
        "            words = [word for word in words if not any(char in emoji.UNICODE_EMOJI for char in word)]\n",
        "            words = [word for word in words if not word in stop_words] # Eliminar stop words\n",
        "            words = [word for word in words if len(word) > 1] # Eliminar letras individuales\n",
        "            words = \" \".join(words)\n",
        "\n",
        "            # Agregar los datos del tweet a la lista de tweets\n",
        "            tweets_list.append([user, text, words, date, hashtags, likes, retweets, tweet_id])\n",
        "\n",
        "# Crear el dataframe a partir de la lista de tweets\n",
        "df = pd.DataFrame(tweets_list, columns=[\"Usuario\", \"Tweet\", \"Palabras_sin_stopwords\", \"Fecha\", \"Hashtags\", \"Likes\", \"Retweets\", \"Tweet_ID\"])\n",
        "\n",
        "# Guardar el dataframe como un archivo CSV en Google Drive\n",
        "drive.mount('/content/drive')\n",
        "csv_path = '/content/drive/MyDrive/datos_twitter/tweets_senadores3.csv'\n",
        "df.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ief7dfzRf0Bf",
        "outputId": "00edfa67-1c0c-45b2-a51e-3eee4aa2fe1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.api:Rate limit reached. Sleeping for: 203\n",
            "WARNING:tweepy.api:Rate limit reached. Sleeping for: 588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users4 = [\"KarimBianchi\",\"KuschelSenador\",\"lagosweber\",\"lcruzcoke\",\"loretosenadora\",\"Luzebensperger\",\"matiaswalkerp\",\"MjGaticaB\",\"mjossandon\",\"paulinanu\"]"
      ],
      "metadata": {
        "id": "6-se0NFHfDph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recopilar los tweets de los usuarios en un dataframe\n",
        "tweets_list = []\n",
        "\n",
        "for user in users4:\n",
        "    # Excluir retweets\n",
        "    tweets = tweepy.Cursor(api.user_timeline, screen_name=user, tweet_mode=\"extended\", exclude_replies=True).items()\n",
        "    for tweet in tweets:\n",
        "        # Verificar si el tweet fue publicado después de la fecha de inicio\n",
        "        if tweet.created_at.replace(tzinfo=timezone.utc) >= pd.to_datetime(start_date).replace(tzinfo=timezone.utc):\n",
        "            # Recopilar la información del tweet\n",
        "            user = tweet.user.screen_name\n",
        "            text = tweet.full_text\n",
        "            date = tweet.created_at\n",
        "            hashtags = [tag['text'] for tag in tweet.entities['hashtags']]\n",
        "            likes = tweet.favorite_count\n",
        "            retweets = tweet.retweet_count\n",
        "            tweet_id = tweet.id_str\n",
        "\n",
        "            # Preprocesamiento del texto\n",
        "            text = text.lower()\n",
        "            text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text) # Eliminar menciones\n",
        "            text = re.sub(r\"http\\S+\", \"\", text) # Eliminar enlaces\n",
        "            text = re.sub(r\"\\d+\", \"\", text) # Eliminar números\n",
        "            words = re.findall(r'\\w+|[¿¡]+|\\S', text)\n",
        "            words = [word.translate(str.maketrans(\"\", \"\", string.punctuation)) if word.isalpha() else word for word in words]\n",
        "            words = [word for word in words if not any(char in emoji.UNICODE_EMOJI for char in word)]\n",
        "            words = [word for word in words if not word in stop_words] # Eliminar stop words\n",
        "            words = [word for word in words if len(word) > 1] # Eliminar letras individuales\n",
        "            words = \" \".join(words)\n",
        "\n",
        "            # Agregar los datos del tweet a la lista de tweets\n",
        "            tweets_list.append([user, text, words, date, hashtags, likes, retweets, tweet_id])\n",
        "\n",
        "# Crear el dataframe a partir de la lista de tweets\n",
        "df = pd.DataFrame(tweets_list, columns=[\"Usuario\", \"Tweet\", \"Palabras_sin_stopwords\", \"Fecha\", \"Hashtags\", \"Likes\", \"Retweets\", \"Tweet_ID\"])\n",
        "\n",
        "# Guardar el dataframe como un archivo CSV en Google Drive\n",
        "drive.mount('/content/drive')\n",
        "csv_path = '/content/drive/MyDrive/datos_twitter/tweets_senadores4.csv'\n",
        "df.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpylXeSEf8GS",
        "outputId": "89a48e21-e742-45e0-bf3e-cb0d29f52ea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.api:Rate limit reached. Sleeping for: 216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users5 = [\"ProvosteYasna\",\"PughSenador\",\"RafaelProhensE\",\"RojoEdwards\",\"sandovalplaza\",\"sekeitel\",\"SenadorCastro\",\"SenadorJSoria\",\"senadorLatorre\"]"
      ],
      "metadata": {
        "id": "U_lJvAotfQte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recopilar los tweets de los usuarios en un dataframe\n",
        "tweets_list = []\n",
        "\n",
        "for user in users5:\n",
        "    # Excluir retweets\n",
        "    tweets = tweepy.Cursor(api.user_timeline, screen_name=user, tweet_mode=\"extended\", exclude_replies=True).items()\n",
        "    for tweet in tweets:\n",
        "        # Verificar si el tweet fue publicado después de la fecha de inicio\n",
        "        if tweet.created_at.replace(tzinfo=timezone.utc) >= pd.to_datetime(start_date).replace(tzinfo=timezone.utc):\n",
        "            # Recopilar la información del tweet\n",
        "            user = tweet.user.screen_name\n",
        "            text = tweet.full_text\n",
        "            date = tweet.created_at\n",
        "            hashtags = [tag['text'] for tag in tweet.entities['hashtags']]\n",
        "            likes = tweet.favorite_count\n",
        "            retweets = tweet.retweet_count\n",
        "            tweet_id = tweet.id_str\n",
        "\n",
        "            # Preprocesamiento del texto\n",
        "            text = text.lower()\n",
        "            text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text) # Eliminar menciones\n",
        "            text = re.sub(r\"http\\S+\", \"\", text) # Eliminar enlaces\n",
        "            text = re.sub(r\"\\d+\", \"\", text) # Eliminar números\n",
        "            words = re.findall(r'\\w+|[¿¡]+|\\S', text)\n",
        "            words = [word.translate(str.maketrans(\"\", \"\", string.punctuation)) if word.isalpha() else word for word in words]\n",
        "            words = [word for word in words if not any(char in emoji.UNICODE_EMOJI for char in word)]\n",
        "            words = [word for word in words if not word in stop_words] # Eliminar stop words\n",
        "            words = [word for word in words if len(word) > 1] # Eliminar letras individuales\n",
        "            words = \" \".join(words)\n",
        "\n",
        "            # Agregar los datos del tweet a la lista de tweets\n",
        "            tweets_list.append([user, text, words, date, hashtags, likes, retweets, tweet_id])\n",
        "\n",
        "# Crear el dataframe a partir de la lista de tweets\n",
        "df = pd.DataFrame(tweets_list, columns=[\"Usuario\", \"Tweet\", \"Palabras_sin_stopwords\", \"Fecha\", \"Hashtags\", \"Likes\", \"Retweets\", \"Tweet_ID\"])\n",
        "\n",
        "# Guardar el dataframe como un archivo CSV en Google Drive\n",
        "drive.mount('/content/drive')\n",
        "csv_path = '/content/drive/MyDrive/datos_twitter/tweets_senadores5.csv'\n",
        "df.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYVWFi-wgCuR",
        "outputId": "a208283d-da05-4f08-d5c2-0da62eb25698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tweepy.api:Rate limit reached. Sleeping for: 570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users6 = [\"senadorquintana\",\"sergiogahona\",\"vanrysselberghe\",\"ximenaordenes\",\"ximerincon\"]"
      ],
      "metadata": {
        "id": "2w0PY5g4fQkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recopilar los tweets de los usuarios en un dataframe\n",
        "tweets_list = []\n",
        "\n",
        "for user in users6:\n",
        "    # Excluir retweets\n",
        "    tweets = tweepy.Cursor(api.user_timeline, screen_name=user, tweet_mode=\"extended\", exclude_replies=True).items()\n",
        "    for tweet in tweets:\n",
        "        # Verificar si el tweet fue publicado después de la fecha de inicio\n",
        "        if tweet.created_at.replace(tzinfo=timezone.utc) >= pd.to_datetime(start_date).replace(tzinfo=timezone.utc):\n",
        "            # Recopilar la información del tweet\n",
        "            user = tweet.user.screen_name\n",
        "            text = tweet.full_text\n",
        "            date = tweet.created_at\n",
        "            hashtags = [tag['text'] for tag in tweet.entities['hashtags']]\n",
        "            likes = tweet.favorite_count\n",
        "            retweets = tweet.retweet_count\n",
        "            tweet_id = tweet.id_str\n",
        "\n",
        "            # Preprocesamiento del texto\n",
        "            text = text.lower()\n",
        "            text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text) # Eliminar menciones\n",
        "            text = re.sub(r\"http\\S+\", \"\", text) # Eliminar enlaces\n",
        "            text = re.sub(r\"\\d+\", \"\", text) # Eliminar números\n",
        "            words = re.findall(r'\\w+|[¿¡]+|\\S', text)\n",
        "            words = [word.translate(str.maketrans(\"\", \"\", string.punctuation)) if word.isalpha() else word for word in words]\n",
        "            words = [word for word in words if not any(char in emoji.UNICODE_EMOJI for char in word)]\n",
        "            words = [word for word in words if not word in stop_words] # Eliminar stop words\n",
        "            words = [word for word in words if len(word) > 1] # Eliminar letras individuales\n",
        "            words = \" \".join(words)\n",
        "\n",
        "            # Agregar los datos del tweet a la lista de tweets\n",
        "            tweets_list.append([user, text, words, date, hashtags, likes, retweets, tweet_id])\n",
        "\n",
        "# Crear el dataframe a partir de la lista de tweets\n",
        "df = pd.DataFrame(tweets_list, columns=[\"Usuario\", \"Tweet\", \"Palabras_sin_stopwords\", \"Fecha\", \"Hashtags\", \"Likes\", \"Retweets\", \"Tweet_ID\"])\n",
        "\n",
        "# Guardar el dataframe como un archivo CSV en Google Drive\n",
        "drive.mount('/content/drive')\n",
        "csv_path = '/content/drive/MyDrive/datos_twitter/tweets_senadores6.csv'\n",
        "df.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "id": "vWNNjbuygFid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "6Ldahxtj-LMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/drive/MyDrive/datos_twitter/tweets_senadores6.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/datos_twitter/tweets_senadores5.csv')\n",
        "df3 = pd.read_csv('/content/drive/MyDrive/datos_twitter/tweets_senadores4.csv')\n",
        "df4 = pd.read_csv('/content/drive/MyDrive/datos_twitter/tweets_senadores3.csv')\n",
        "df5 = pd.read_csv('/content/drive/MyDrive/datos_twitter/tweets_senadores2.csv')\n",
        "df6 = pd.read_csv('/content/drive/MyDrive/datos_twitter/tweets_senadores.csv')"
      ],
      "metadata": {
        "id": "Im_a9wUh-Ifr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Concatenar los DataFrames\n",
        "df = pd.concat([df1, df2, df3, df4, df5, df6], ignore_index=True)\n",
        "\n",
        "# Guardar el DataFrame como un archivo CSV en Google Drive\n",
        "csv_path = '/content/drive/MyDrive/datos_twitter/tweets_senadores_total.csv'\n",
        "df.to_csv(csv_path, index=False)"
      ],
      "metadata": {
        "id": "uvRMjqXy-KGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener la fecha actual\n",
        "fecha_actual = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Ruta del archivo original en Google Drive\n",
        "archivo_original = '/content/drive/MyDrive/datos_twitter/tweets_senadores_total.csv'\n",
        "\n",
        "# Ruta de la carpeta donde se guardará el respaldo\n",
        "carpeta_respaldo = '/content/drive/MyDrive/datos_twitter/repositorio_twitter/senado/'\n",
        "\n",
        "# Nombre del archivo de respaldo con la fecha actual\n",
        "nombre_respaldo = 'tweets_senadores_' + fecha_actual + '.csv'\n",
        "\n",
        "# Copiar el archivo original en la carpeta de respaldo con el nombre actualizado\n",
        "!cp \"$archivo_original\" \"$carpeta_respaldo$nombre_respaldo\"\n",
        "\n"
      ],
      "metadata": {
        "id": "j9G6l-z2-Or4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de usuarios de Twitter\n",
        "users_total = [\"adeurresti\",\"AKusanovicG\",\"alvaroelizalde\",\"ArayaPedro\",\"CalamaVelasquez\",\"CarmenAravenaA1\",\"chahuan\",\"Claupascualgrau\",\"daniel_nunez_a\",\"DignidadFabiola\",\"DoctorJLCastro\",\"Duranasenador\",\"felipekast\",\"fidelsenador\",\"galilea_rodrigo\",\"gastonsaavedra\",\"gustavosanhuez\",\"iallendebussi\",\"ifloressenador\",\"Insulza\",\"ivanmoreirab\",\"jacoloma\",\"javiermacaya\",\"jgarciaruminot\",\"KarimBianchi\",\"KuschelSenador\",\"lagosweber\",\"lcruzcoke\",\"loretosenadora\",\"Luzebensperger\",\"matiaswalkerp\",\"MjGaticaB\",\"mjossandon\",\"paulinanu\",\"ProvosteYasna\",\"PughSenador\",\"RafaelProhensE\",\"RojoEdwards\",\"sandovalplaza\",\"sekeitel\",\"SenadorCastro\",\"SenadorJSoria\",\"senadorLatorre\",\"senadorquintana\",\"sergiogahona\",\"vanrysselberghe\",\"ximenaordenes\",\"ximerincon\"]"
      ],
      "metadata": {
        "id": "tLUMvlR_-Se1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer el archivo CSV previo con los datos existentes\n",
        "csv_path = '/content/drive/MyDrive/datos_twitter/tweets_senadores_total.csv'\n",
        "df_previo = pd.read_csv(csv_path)\n",
        "\n",
        "# Obtener la fecha del último tweet registrado en el archivo previo\n",
        "last_date = pd.to_datetime(df_previo['Fecha']).max().replace(tzinfo=timezone.utc)\n",
        "\n",
        "# Recopilar los tweets de los usuarios en un dataframe\n",
        "tweets_list = []\n",
        "\n",
        "for user in users:\n",
        "    # Excluir retweets\n",
        "    tweets = tweepy.Cursor(api.user_timeline, screen_name=user, tweet_mode=\"extended\", exclude_replies=True).items()\n",
        "    for tweet in tweets:\n",
        "        # Verificar si el tweet fue publicado después de la fecha de inicio\n",
        "         if tweet.created_at.replace(tzinfo=timezone.utc) >= pd.to_datetime(start_date).replace(tzinfo=timezone.utc):\n",
        "            # Recopilar la información del tweet\n",
        "            user = tweet.user.screen_name\n",
        "            text = tweet.full_text\n",
        "            date = tweet.created_at\n",
        "            hashtags = [tag['text'] for tag in tweet.entities['hashtags']]\n",
        "            likes = tweet.favorite_count\n",
        "            retweets = tweet.retweet_count\n",
        "            tweet_id = tweet.id_str\n",
        "\n",
        "            # Preprocesamiento del texto\n",
        "            text = text.lower()\n",
        "            text = re.sub(r\"@[A-Za-z0-9]+\", \"\", text) # Eliminar menciones\n",
        "            text = re.sub(r\"http\\S+\", \"\", text) # Eliminar enlaces\n",
        "            text = re.sub(r\"\\d+\", \"\", text) # Eliminar números\n",
        "            words = re.findall(r'\\w+|[¿¡]+|\\S', text)\n",
        "            words = [word.translate(str.maketrans(\"\", \"\", string.punctuation)) if word.isalpha() else word for word in words]\n",
        "            words = [word for word in words if not any(char in emoji.UNICODE_EMOJI for char in word)]\n",
        "            words = [word for word in words if not word in stop_words] # Eliminar stop words\n",
        "            words = [word for word in words if len(word) > 1] # Eliminar letras individuales\n",
        "            words = \" \".join(words)\n",
        "\n",
        "            # Agregar los datos del tweet a la lista de tweets\n",
        "            tweets_list.append([user, text, words, date, hashtags, likes, retweets, tweet_id])\n",
        "\n",
        "# Crear el dataframe a partir de la lista de tweets\n",
        "new_df = pd.DataFrame(tweets_list, columns=[\"Usuario\", \"Tweet\", \"Palabras_sin_stopwords\", \"Fecha\", \"Hashtags\", \"Likes\", \"Retweets\", \"Tweet_ID\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "onqSnBLF-V_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_GYt6rc_63iE"
      }
    }
  ]
}